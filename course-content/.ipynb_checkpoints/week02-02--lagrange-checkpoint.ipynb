{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [George McNinch](http://gmcninch.math.tufts.edu) Math 87 - Spring 2024\n",
    "\n",
    "\n",
    "# Week 2: Lagrange multpliers \n",
    "\n",
    "(multivariable optimization, continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some motivation\n",
    "\n",
    "Let's recall our television manufacturing example for a moment. The model we solved was interesting \n",
    "but most likely unrealistic. A manufacturer, for instance, probably has a limited\n",
    "capacity and can only produce a certain amount of TVs in total per year. \n",
    "\n",
    "Let’s look at an example where that capacity is 10,000 TVs per year.\n",
    "\n",
    "So given the *constraint*, $s + t ≤ 10,000$, how many of each model should they produce now??\n",
    "\n",
    "Well, first notice that the *unconstrained optimum* with $s = 4,735$ and $t = 7,043$ -- found *previously* --\n",
    "does not satisfy the constraint (since $s + t = 4735 + 7043 > 11700$).\n",
    "\n",
    "Now let's recall that in the single variable case, trying to find a *constrained optimum* amounts to optimizing a function on a closed interval -- so you proceed with the usual procedure for optimization but must also check boundary points (endpoints).\n",
    "\n",
    "If we proceed in this fashion, we'd check the \"boundary\" conditions corresponding to $t=0$ and $s=0$.\n",
    "\n",
    "Well, setting $t=0$ we see that the profit function is given by\n",
    "\n",
    "$$P(s,0) = 144s - 0.01s^2 - 400,000.$$\n",
    "\n",
    "Since this is a function only of $s$ we can use 1-dimensional optimization techniques:\n",
    "\n",
    "$$\\dfrac{\\partial P}{\\partial S}(S,0) = 144 - 0.02s = 0 \\implies s = 7200.$$\n",
    "\n",
    "On this boundary, we need to consider $(0,0)$ and $(10,000,0)$ (the boundary of the boundary...) as well as $(7200,0)$.\n",
    "\n",
    "We can treat the boundary with $s =0$ similarly.\n",
    "\n",
    "But the boundary condition with $s+t = 0$ will be more of a pain.\n",
    "\n",
    "The method of Lagrange multipliers gives a more systematic way to proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange multipliers\n",
    "\n",
    "Consider a function $f(x,y)$ of two variables. We are interested here in finding maximal or minimal values of $f$ subject to a *constraint*. The sort of constraint we have in mind is a restriction on the possible pairs $(x,y)$ -- so we have a second function $g(x,y)$ and we want to maximize (or minimize) $f$ subject\n",
    "to the condition that $g(x,y) = c$ for some fixed quantity $c$.\n",
    "\n",
    "We introduce a \"new\" function -- now of *three* variables - known as the **Lagrangian**. It is given by the formula\n",
    "$$\\mathcal{L}(x,y,\\lambda) = f(x,y) - \\lambda \\cdot (g(x,y)-c)$$\n",
    "\n",
    "(We use the Greek letter $\\lambda$ for our third variable in part because it plays a different role for us than the variables $x,y$).\n",
    "\n",
    "We can calculate the *partial derivatives* of this Lagrangian; they are:\n",
    "\n",
    "$$\\dfrac{\\partial \\mathcal{L}}{\\partial x} = \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x}$$\n",
    "\n",
    "$$\\dfrac{\\partial \\mathcal{L}}{\\partial y} = \\dfrac{\\partial f}{\\partial y} - \\lambda\\dfrac{\\partial g}{\\partial y}$$\n",
    "\n",
    "$$\\dfrac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(g(x,y)-c)$$\n",
    "\n",
    "If we seek critical points of the Lagrangian, we therefore must require\n",
    "$$0 = \\dfrac{\\partial \\mathcal{L}}{\\partial x} = \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x}$$\n",
    "and similarly for $y$, so that\n",
    "$$ \\dfrac{\\partial f}{\\partial x} = \\lambda \\dfrac{\\partial g}{\\partial x} \\quad \\text{and}\\quad\n",
    "\\dfrac{\\partial f}{\\partial y} = \\lambda \\dfrac{\\partial g}{\\partial y}.$$\n",
    "Recall that the *gradient* $\\nabla f$ of $f$ is given by $\\nabla f = \\dfrac{\\partial f}{\\partial x} \\mathbf{i} + \\dfrac{\\partial f}{\\partial y} \\mathbf{j}$ -- or $\\nabla f = \\left(\\dfrac{\\partial f}{\\partial x},\\dfrac{\\partial f}{\\partial y}\\right)$ -- so these conditions amount to:\n",
    "\n",
    "$$\\nabla f = \\lambda \\cdot \\nabla \\mathcal{L}$$\n",
    "\n",
    "Moreover, we find that\n",
    "$$0 = \\dfrac{\\partial \\mathcal{L}}{\\partial \\lambda} = c - g(x,y).$$\n",
    "\n",
    "Summarizing, we record\n",
    "\n",
    "**Proposition**\n",
    "  : The condition that $(x_0,y_0,\\lambda_0)$ is a critical point of $F$ is equivalent to two requirements: \n",
    "  \n",
    "  **(a).** $(x_0,y_0)$ must be on the level curve $g(x,y) = c$, and\n",
    "     \n",
    "  **(b).** the gradient vectors must satisfy $\\nabla f \\vert_{(x_0,y_0)} = \\lambda_0 \\nabla g \\vert_{(x_0,y_0)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for interest in the critical points of $\\mathcal{L}$ is the following:\n",
    "\n",
    "**Proposition** \n",
    "  : Optimal values for $f$ along the level curve $g(x,y) = c$ will be found among the critical points of $F$. \n",
    "\n",
    "Indeed, suppose $(x_0,y_0)$\n",
    "is a point on the level curve at which $f$ takes its max (or min) value (on the level surface).\n",
    "We need to argue that the gradient vector $\\nabla f \\vert_{(x_0,y_0)}$ is \"parallel\" to the gradient\n",
    "vector $\\nabla g \\vert_{(x_0,y_0)}$ -- i.e. that **(b)** above holds.\n",
    "\n",
    "Well, we can of course write $\\nabla f \\vert_{(x_0,y_0)} = \\mathbf{v} + \\mu \\nabla g \\vert_{(x_0,y_0)}$\n",
    "for a vector $\\mathbf{v}$ perpendicular to $\\nabla g \\vert_{(x_0,y_0)}$ (and for some scalar $\\mu$). To confirm that **(b)** holds,\n",
    "we must argue that $\\mathbf{v}$ is zero.\n",
    "\n",
    "But if $\\mathbf{v}$ is non-zero, then walking along the level curve $g(x,y) = c$ \"in the direction of $\\mathbf{v}$\" \n",
    "will lead to points at which the function $f$ will take larger values, contrary to the assumption that on the level curve, $f$ has a maximum at $(x_0,y_0)$. \n",
    "$\\quad \\blacksquare$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Maxima (or minima) and an interpretation of $\\lambda$\n",
    "\n",
    "Haivng found the critical points of $F$, one can find the extrema for $f$ simple by evaluating and comparing the results (*well, this works at least if there are just a finite list of critical points*).\n",
    "\n",
    "If $M^*$ denotes the max (or min) value of $f$ subject to the constraint $g(x,y)=c$, we can view $F^*$ as a function $F^*(c)$ of $c$. (Different values of $c$ result in different extrema for $f$ on the curve $g(x,y) = c$...)\n",
    "\n",
    "We now claim that if $(x_0,y_0,\\lambda_0)$ is a critical point for $F$ for which $F^* = f(x_0,y_0)$, then\n",
    "$$\\dfrac{dF^*}{dc} = \\lambda_0.$$\n",
    "\n",
    "We are first going to consider this observation in the case of an *application*. Then we'll explain why this observation is true in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Televisions, again\n",
    "-------------------\n",
    "\n",
    "Let’s return to the television manufacturing problem. We consider the constraint $$g(s, t) = s + t = 10,000$$\n",
    "i.e. \"the manufacturer produces exactly 10,000 televisions\".\n",
    "\n",
    "Consider the *Lagrangian* function $\\mathcal{L}(s,t,\\lambda) = P(s,t) - \\lambda(s+t-10,000).$\n",
    "\n",
    "Looking for critical points of $\\mathcal{L}$, we find that:\n",
    "\n",
    "$$ \\left \\{ \n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial P}{\\partial s}  - \\lambda \\dfrac{\\partial g}{\\partial s} &=& 144 − 0.02s − 0.007t − λ &= 0 \\\\\n",
    "\\dfrac{\\partial P}{\\partial t}  - \\lambda \\dfrac{\\partial g}{\\partial t} &=&  174 − 0.007s − 0.002t − λ &= 0 \\\\\n",
    "g(s,t) - c &=& -10000 +s +t &=  0\n",
    "\\end{matrix}\n",
    "\\right .$$\n",
    "\n",
    "This leads to 3 linear equations in 3 unknowns, which we can easily solve bny hand. Or we can use `python` and `numpy` as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3846.15384615, 6153.84615385,   24.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## coefficient matrix\n",
    "M=np.array([[0.02,.007,1],[.007,.02,1],[1,1,0]])\n",
    "\n",
    "b=np.array([144,174,10000])\n",
    "\n",
    "np.linalg.solve(M,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we find that\n",
    "$s \\approx 3846$, $t \\approx 6154$\n",
    "and $λ = 24$.\n",
    "\n",
    "\n",
    "\n",
    "Now, remember that we want to maximize the profit function $P$ subject to the constraint\n",
    "$s+t=10,000$ where $s \\ge 0$ and $t \\ge 0$. On this \"closed interval\", the function $P$ will assume a maximum and a minimum value. We've found the critical point of $P$ on this \"interval\" -- namely $(3846, 6154)$. The *endpoints* of the interval are $(0,10000)$ and $(10000,0)$.\n",
    "\n",
    "\n",
    "Let's compare the values of $P$ at these three points of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340000.0, 532307.692, 40000.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def p(s,t):\n",
    "    return -400000 + 144*s + 174*t - 0.01*s**2 - 0.01*t**2 - .007*s*t\n",
    "\n",
    "ss = [ 0, \n",
    "       3846, \n",
    "       10000\n",
    "     ]\n",
    "tt = [ 10000, \n",
    "       6154, \n",
    "       0\n",
    "     ]\n",
    "\n",
    "list(map(p,ss,tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the *maximum constrained profit* is\n",
    "$$P^* = P(3846 , 6154) \\approx 532308.$$\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "**Q:** What choice of $s,t$ give the *minimum* constrained profit? \n",
    "\n",
    "(Could you have guessed that *a priori*?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shadow prices and an interpretation for the \"multiplier\" $\\lambda$\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "Let's carry out *sensitivity analysis* on the value of the *constraint*.\n",
    "\n",
    "Recall that our constraint is $g(s,t) = s+t = 10000$. So we instead set\n",
    "$$g(s,t) = s + t = c.$$\n",
    "\n",
    "In this case, we must instead solve the system of equations\n",
    "$Mx = b$ where\n",
    "$$M = \\begin{pmatrix} 0.02 & 0.007 & 1 \\\\\n",
    "0.007 & 0.02 & 1 \\\\\n",
    "1 & 1 & 0 \n",
    "\\end{pmatrix} \\quad \\text{and} \\quad\n",
    "b = \\begin{pmatrix}\n",
    " 144 \\\\ 174 \\\\ c\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "Since the vector $b$ has an \"unknown\" coefficient, it isn't clear how to use `numpy`'s ``linalg.solve`` method. We can circumvent this by *inverting* the coefficient matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02  0.007 1.   ]\n",
      " [0.007 0.02  1.   ]\n",
      " [1.    1.    0.   ]]\n",
      "[[ 3.84615385e+01 -3.84615385e+01  5.00000000e-01]\n",
      " [-3.84615385e+01  3.84615385e+01  5.00000000e-01]\n",
      " [ 5.00000000e-01  5.00000000e-01 -1.35000000e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "import pprint as pp\n",
    "\n",
    "## coefficient matrix\n",
    "M=np.array([[0.02,0.007,1],[0.007,0.02,1],[1,1,0]])\n",
    "\n",
    "## inverse of coefficient matrix\n",
    "Mi = np.linalg.inv(M)\n",
    "\n",
    "print(M)\n",
    "print(Mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We find the result by the computation $\\mathbf{x} = M^{-1} \\mathbf{b}$.\n",
    " \n",
    "In order to represent the vector $\\mathbf{b} = \\begin{pmatrix} 144 \\\\ 174 \\\\ c \\end{pmatrix}$, we first use `sympy` to introduce a ``symbol`` for the unknown ``c``.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = 0.5*c - 1153.84615384615\n",
      "t = 0.5*c + 1153.84615384615\n",
      "lambda = 159.0 - 0.0135*c\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = sp.Symbol('c')\n",
    "b=np.array([144,174,c])\n",
    "\n",
    "#res=np.matmul(Mi,b)  # which we can also write as follows\n",
    "res = Mi @ b\n",
    "\n",
    "print(f\"s = {res[0]}\")\n",
    "print(f\"t = {res[1]}\")\n",
    "print(f\"lambda = {res[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the solution has\n",
    "$$s = \\dfrac{c}{2} - 1153.85 \\qquad t = \\dfrac{c}{2} + 1153.85 \\qquad \\text{and} \\qquad \\lambda = 159 \n",
    "- 0.0135c.$$\n",
    "\n",
    "Thus $\\dfrac{ds}{dc} = \\dfrac{dt}{dc} = \\dfrac{1}{2}$. We can compute the sensitivity \n",
    "$$S(s,c=10000) \\approx 1.3$$\n",
    "of $s$ as a function of $c$, and \n",
    "$$S(t,c=10000) \\approx 0.8$$\n",
    "pf $t$ as a function of $t$. In particular, an increase in the maximum production level of TVs (i.e. an increase in $c$) will result in an increase in the optimal production levels $s$ and $t$.\n",
    "\n",
    "What about the sensitivity $S(P,c) = \\dfrac{dP}{dc} \\cdot \\dfrac{c}{P}$? For this, we first need to compute $\\dfrac{dP}{dc}$.\n",
    "\n",
    "To compute $\\dfrac{dP}{dc}$ we can use the several-variable chain rule\n",
    "$$\\dfrac{dP}{dc} = \\dfrac{\\partial P}{\\partial s} \\dfrac{ds}{dc} + \\dfrac{\\partial P}{\\partial t} \\dfrac{dt}{dc},$$ or just rewrite\n",
    "$P$ as a function of $c$. \n",
    "Interestingly, observe that *at the critical point* on the curve $g(s,t) = c$, the partial derivatives of $P$ satisfy\n",
    "$$\\dfrac{\\partial P}{\\partial s} = \\lambda \\dfrac{\\partial g}{\\partial s} = \\lambda\n",
    "\\quad \\text{and} \\quad\n",
    "\\dfrac{\\partial P}{\\partial t} = \\lambda \\dfrac{\\partial g}{\\partial t} = \\lambda\n",
    "$$\n",
    "(since $g(s,t) = s+t$).\n",
    "\n",
    "Since $\\dfrac{ds}{dc} = \\dfrac{dt}{dc} = 1/2$, conclude that $$\\dfrac{dP}{dc} = \\lambda.$$\n",
    "\n",
    "This confirms the formula we mentioned previously. In fact, as we pointed out, this formula holds in general. See the discussion at the end of this notebook, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the *sensitivity* of the profit to the parameter $c$:\n",
    "$$S(P,c=10000)  \\dfrac{dP}{dc} \\cdot \\dfrac{c}{P} \\approx 24 \\cdot \\dfrac{10000}{532308} \\approx 0.45.$$\n",
    "So a 1% increase in $c$ yields a $0.45%$ increase in $P$.\n",
    "\n",
    "\n",
    "Thus, the Lagrange multiplier, λ actually has a *concrete* meaning here as a *shadow price*. If you are allowed to\n",
    "produce more TV’s it tells you how much that change affects your profit. Therefore, if the cost of\n",
    "making the change in production level is less than the additional profit, you probably should go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting $\\lambda$, revisited\n",
    "\n",
    "Lets return to our general setting: $f$ is a function $f(x,y)$ of two variables that we want to optimize,\n",
    "and our constraint is given by an equation $g(x,y) = c$.\n",
    "\n",
    "Recall that the Lagrangian $\\mathcal{L}$ is given by\n",
    "$$\\mathcal{L} = f(x,y) - \\lambda(g(x,y) - c).$$\n",
    "\n",
    "Let's suppose that - as we've described above - for a given $c$, the maximum value of $f$ is determined\n",
    "by the critical point $(x^*(c),y^*(c),\\lambda^*(c))$ of the Lagrangian $\\mathcal{L}$.\n",
    "\n",
    "Now, let's view the Lagrangian as a function of the *four* variables\n",
    "$$\\mathcal{L}(x,y,\\lambda,c).$$\n",
    "\n",
    "We first note that\n",
    "$$\\dfrac{\\partial \\mathcal{L}}{\\partial c} = \\dfrac{\\partial}{\\partial c}\\left[f(x,y) - \\lambda(g(x,y) - c)\\right] = \\lambda.$$\n",
    "\n",
    "Since $(x^*(c),y^*(c),\\lambda^*(c))$ is a critical point of the Lagrangian, we know that $g(x^*(c),y^*(c)) - c = 0$, so that\n",
    "$$F^*(c) = \\mathcal{L}(x^*(c),y^*(c),\\lambda^*(c),c).$$\n",
    "\n",
    "Now use the multi-variate chain rule we see that\n",
    "$$\\dfrac{dF^*}{dc} = \n",
    "\\dfrac{\\partial{\\mathcal{L}}}{\\partial x}\\dfrac{dx^*}{dc}\n",
    "+ \\dfrac{\\partial{\\mathcal{L}}}{\\partial y}\\dfrac{dy^*}{dc}\n",
    "+ \\dfrac{\\partial{\\mathcal{L}}}{\\partial \\lambda} \\dfrac{d\\lambda^*}{dc}\n",
    "+\\dfrac{\\partial{\\mathcal{L}}}{\\partial c}$$\n",
    "\n",
    "We now notice that the partial derivatives\n",
    "$\\dfrac{\\partial \\mathcal{L}}{\\partial x}$,\n",
    "$\\dfrac{\\partial \\mathcal{L}}{\\partial y}$,\n",
    "$\\dfrac{\\partial \\mathcal{L}}{\\partial \\lambda}$\n",
    "are all zero when evaluated at $(x^*(c),y^*(c),\\lambda^*(c)$.\n",
    "\n",
    "This confirms that\n",
    "$$\\dfrac{dF^*}{dc} = \\dfrac{\\partial{\\mathcal{L}}}{\\partial c} = \\lambda$$\n",
    "as we asserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
